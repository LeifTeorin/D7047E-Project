# -*- coding: utf-8 -*-
"""LSTM hate speech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y0ixivYo_NsnvGv-yaogqxX_EmiY0V5X
"""

!pip install wandb

# PREPROCESSING FOR TASK 1

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset into a Pandas dataframe
df = pd.read_csv('english_dataset.tsv', sep= '\t')
print(df.columns)

# Remove unwanted columns
df.drop(['task_2', 'task_3'], axis=1, inplace=True)

print(df.columns)
# Remove URLs and mentions
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"http\S+", "", x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"@\S+", "", x))

# Convert text to lowercase
df['tweet'] = df['tweet'].apply(lambda x: x.lower())

# Remove non-alphanumeric characters
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

# Remove stopwords
stop_words = set(stopwords.words('english'))
df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Lemmatize words
# lemmatizer = WordNetLemmatizer()
# df['tweet'] = df['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Encode labels as integers
label_map = {'NOT': 0, 'HOF': 1}
df['label'] = df['task_1'].apply(lambda x: label_map[x])

# Save preprocessed dataset to a new CSV file
df.to_csv('preprocessed_hasoc_dataset1.csv', index=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import wandb
from wandb.keras import WandbCallback

wandb.init(project="adl_2023", name="lstm_task1", config={
    "batch_size": 32,
    "epochs": 6
})

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset1.csv')


# Preprocessing
X = df['tweet'].values
y = df['task_1'].replace({'HOF': 1, 'NOT': 0}).values

# Split into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the text
tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Padding
max_len = 100
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
X_test = pad_sequences(X_test, padding='post', maxlen=max_len)

# Model
model = Sequential()
model.add(Embedding(5000, 128, input_length=X_train.shape[1]))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

#optimizer = tf.keras.optimizers.AdamW(learning_rate=0.00004)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Training
history = model.fit(X_train, y_train, batch_size=wandb.config.batch_size, epochs=wandb.config.epochs, validation_split=0.1, callbacks=[WandbCallback()])

# Testing
y_pred_prob = model.predict(X_test)
y_pred = np.where(y_pred_prob > 0.5, 1, 0)

# Evaluation
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_pred))
print('Classification Report:', classification_report(y_test, y_pred))

# PREPROCESSING FOR TASK 2

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset into a Pandas dataframe
df = pd.read_csv('english_dataset.tsv', sep= '\t')
print(df.columns)

# Remove unwanted columns
df.drop(['task_1', 'task_3'], axis=1, inplace=True)

print(df.columns)
# Remove URLs and mentions
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"http\S+", "", x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"@\S+", "", x))

# Convert text to lowercase
df['tweet'] = df['tweet'].apply(lambda x: x.lower())

# Remove non-alphanumeric characters
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

# Remove stopwords
stop_words = set(stopwords.words('english'))
df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Lemmatize words
# lemmatizer = WordNetLemmatizer()
# df['tweet'] = df['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Encode labels as integers
label_map = {'NONE': 0, 'HATE': 1, 'PRFN':2, 'OFFN':3}
df['label'] = df['task_2'].apply(lambda x: label_map[x])

# Save preprocessed dataset to a new CSV file
df.to_csv('preprocessed_hasoc_dataset_task2.csv', index=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

wandb.init(project="adl_2023", name="lstm_task1.2", config={
    "batch_size": 32,
    "epochs": 6
})

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset_task2.csv')


# Preprocessing
X = df['tweet'].values
y = df['task_2'].replace({'NONE': 0, 'HATE': 1, 'PRFN':2, 'OFFN':3}).values

# Split into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the text
tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Padding
max_len = 100
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
X_test = pad_sequences(X_test, padding='post', maxlen=max_len)

# Model
model = Sequential()
model.add(Embedding(5000, 128, input_length=X_train.shape[1]))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Training
history = model.fit(X_train, y_train, batch_size=wandb.config.batch_size, epochs=wandb.config.epochs, validation_split=0.1, callbacks=[WandbCallback()])

# Testing
y_pred_prob = model.predict(X_test)
y_pred = np.where(y_pred_prob > 0.5, 1, 0)

# Evaluation
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_pred))
print('Classification Report:', classification_report(y_test, y_pred))

# PREPROCESSING FOR TASK 3

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset into a Pandas dataframe
df = pd.read_csv('english_dataset.tsv', sep= '\t')
print(df.columns)

# Remove unwanted columns
df.drop(['task_1', 'task_2'], axis=1, inplace=True)

print(df.columns)
# Remove URLs and mentions
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"http\S+", "", x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"@\S+", "", x))

# Convert text to lowercase
df['tweet'] = df['tweet'].apply(lambda x: x.lower())

# Remove non-alphanumeric characters
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

# Remove stopwords
stop_words = set(stopwords.words('english'))
df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Lemmatize words
# lemmatizer = WordNetLemmatizer()
# df['tweet'] = df['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Encode labels as integers
label_map = {'NONE': 0, 'TIN': 1, 'UNT':2}
df['label'] = df['task_3'].apply(lambda x: label_map[x])

# Save preprocessed dataset to a new CSV file
df.to_csv('preprocessed_hasoc_dataset_task3.csv', index=False)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

wandb.init(project="adl_2023", name="lstm_task1.3", config={
    "batch_size": 32,
    "epochs": 6
})

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset_task3.csv')


# Preprocessing
X = df['tweet'].values
y = df['task_3'].replace({'NONE': 0, 'TIN': 1, 'UNT':2}).values

# Split into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the text
tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Padding
max_len = 100
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
X_test = pad_sequences(X_test, padding='post', maxlen=max_len)

# Model
model = Sequential()
model.add(Embedding(5000, 128, input_length=X_train.shape[1]))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Training
history = model.fit(X_train, y_train, batch_size=wandb.config.batch_size, epochs=wandb.config.epochs, validation_split=0.1, callbacks=[WandbCallback()])

# Testing
y_pred_prob = model.predict(X_test)
y_pred = np.where(y_pred_prob > 0.5, 1, 0)

# Evaluation
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_pred))
print('Classification Report:', classification_report(y_test, y_pred))
# -*- coding: utf-8 -*-
"""CNN hate speech detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15W1qsGgIeRq8E2-KR6eq4zyvuOutTCXG
"""

# PREPROCESSING FOR TASK 1

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset into a Pandas dataframe
df = pd.read_csv('HASOCData/english_dataset.tsv', sep= '\t')
print(df.columns)

# Remove unwanted columns
df.drop(['task_2', 'task_3'], axis=1, inplace=True)

print(df.columns)
# Remove URLs and mentions
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"http\S+", "", x))
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r"@\S+", "", x))

# Convert text to lowercase
df['tweet'] = df['tweet'].apply(lambda x: x.lower())

# Remove non-alphanumeric characters
df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

# Remove stopwords
stop_words = set(stopwords.words('english'))
df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Lemmatize words
# lemmatizer = WordNetLemmatizer()
# df['tweet'] = df['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))

# Encode labels as integers
label_map = {'NOT': 0, 'HOF': 1}
df['label'] = df['task_1'].apply(lambda x: label_map[x])

# Save preprocessed dataset to a new CSV file
#df.to_csv('preprocessed_hasoc_dataset_task1.csv', index=False)

import random
# Data Augmentation
def augment_data(df, augmentation_factor=3):
    augmented_data = []
    for index, row in df.iterrows():
        tweet = row['tweet']
        label = row['label']
        augmented_data.append({'tweet': tweet, 'label': label})
        for _ in range(augmentation_factor):
            augmented_tweet = synonym_replacement(tweet)  # Apply desired augmentation technique here
            augmented_data.append({'tweet': augmented_tweet, 'label': label})
    augmented_df = pd.DataFrame(augmented_data)
    return augmented_df

def synonym_replacement(text, n=3):
    # Synonym Replacement code here
    pass

# Apply Data Augmentation
augmented_df = augment_data(df, augmentation_factor=3)

# Save preprocessed and augmented dataset to a new CSV file
augmented_df.to_csv('preprocessed_augmented_hasoc_dataset_task1.csv', index=False)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import wandb
#import sklearn
from wandb.keras import WandbCallback
from tensorflow.keras.regularizers import l2
import gensim.downloader as api

# Download Word2Vec embeddings
word2vec = api.load('word2vec-google-news-300')
word2vec.save_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)

wandb.init(project="adl_2023", name="cnn_task1_GloVe_dO_Es", config={
    "batch_size": 32,
    "epochs": 8
})

# Download Word2Vec embeddings
word2vec = api.load('word2vec-google-news-300')
word2vec.save_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import wandb
from gensim.models import KeyedVectors
from wandb.keras import WandbCallback
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ModelCheckpoint

wandb.init(project="adl_2023", name="cnn_task1_Word2Vec_dO_Es", config={
    "batch_size": 32,
    "epochs": 12
})

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset_task1.csv', sep='\t')

# Preprocess the data
X = df['tweet'].values
y = df['task_1'].replace({'HOF': 1, 'NOT': 0}).values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the data
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Pad the data
maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Load Word2Vec embeddings
word2vec = KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True, encoding='latin1')

# Create the embedding matrix
embedding_dim = 300
oov_vector = np.random.rand(embedding_dim)
embedding_matrix = np.zeros((tokenizer.num_words, embedding_dim))  # Remove +1 for the OOV token

for word, i in tokenizer.word_index.items():
    if i >= tokenizer.num_words:
        break
    if word in word2vec:
        embedding_matrix[i] = word2vec[word]
    else:
        # Initialize missing word embeddings with the OOV vector
        embedding_matrix[i] = oov_vector

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(tokenizer.num_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=True),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Dropout(0.7),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Dropout(0.8),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=l2(0.5))
])

optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001)

model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Define early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# Define model checkpoint to save the best model
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')

# Train the model
model.fit(
    X_train,
    y_train,
    validation_data=(X_test, y_test),
    epochs=wandb.config.epochs,
    batch_size=wandb.config.batch_size,
    callbacks=[WandbCallback(), early_stopping, checkpoint]
)

# Load the best model
best_model = tf.keras.models.load_model('best_model.h5')

# Load the test dataset
test_data = pd.read_csv('preprocessed_hasoc_test_dataset_task1.csv', sep=',')

# Preprocess the test data
X_test = test_data['tweet'].values
y_test = test_data['task_1'].replace({'HOF': 1, 'NOT': 0}).values

# Tokenize and pad the test data
X_test = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Evaluate the best model on the test data
loss, accuracy = best_model.evaluate(X_test, y_test)
print("Test Loss Task 1:", loss)
print("Test Accuracy Task 1:", accuracy)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset_task2.csv', sep='\t')

# Preprocess the data
X = df['tweet'].values
y = df['task_2'].replace({'NONE': 0, 'HATE': 1, 'PRFN':2, 'OFFN':3}).values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the data
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Pad the data
maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

#word embedding
embeddings_index = {}
embedding_dim = 100  # Change the dimension based on the downloaded file

with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Create the embedding matrix
num_words = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((num_words, embedding_dim))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(5000, 128, input_length=maxlen),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("Loss: ", loss)
print("Accuracy: ", accuracy)

# Load the test dataset
test_data = pd.read_csv('preprocessed_hasoc_test_dataset_task2.csv', sep=',')

# Preprocess the test data
X_test = test_data['tweet'].values
y_test = test_data['task_2'].replace({'NONE': 0, 'HATE': 1, 'PRFN':2, 'OFFN':3}).values

# Tokenize and pad the test data
X_test = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Evaluate the best model on the test data
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss Task 2:", loss)
print("Test Accuracy Task 2:", accuracy)

# Load the dataset
data = pd.read_csv('preprocessed_hasoc_dataset_task3.csv', sep='\t')

# Preprocess the data
X = df['tweet'].values
y = df['task_3'].replace({'NONE': 0, 'TIN': 1, 'UNT':2}).values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the data
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Pad the data
maxlen = 100
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(5000, 128, input_length=maxlen),
    tf.keras.layers.Conv1D(64, 5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("Loss: ", loss)
print("Accuracy: ", accuracy)

# Load the test dataset
test_data = pd.read_csv('preprocessed_hasoc_test_dataset_task3.csv', sep=',')

# Preprocess the test data
X_test = test_data['tweet'].values
y_test = test_data['task_3'].replace({'NONE': 0, 'TIN': 1, 'UNT':2}).values

# Tokenize and pad the test data
X_test = tokenizer.texts_to_sequences(X_test)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Evaluate the best model on the test data
loss, accuracy = best_model.evaluate(X_test, y_test)
print("Test Loss Task 3:", loss)
print("Test Accuracy Task 3:", accuracy)